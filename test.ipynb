{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "import flappy_bird_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # left prob # 1 < left , 0 > right\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg = np.array([1,2,3])\n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_policy(observation, model): # policy gradient -> PG\n",
    "    left_probability = model.predict(observation[np.newaxis]) # probability value between 0, and 1\n",
    "    action = int(np.random.rand() > left_probability) # value {0, 1} # exploration vs exploitation concept\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "Optimize learnable parameters of policy by following the gradients towards higher reward (maximizing reward)\n",
    "\n",
    "## steps\n",
    "1. let the NN play the game multiple times and at every step just calculate the gradients (wrt reward) but dont apply it immidiately.\n",
    "2. Once you have completed several episodes then compute the actions using discounted method.\n",
    "3. result of previous step 2 can +ve or -ve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6645621]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, observation, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prabability = model(observation[np.newaxis])\n",
    "        action = (tf.random.uniform([1,1]) > left_prabability) # True and False\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # \n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prabability)) \n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # dc/dw\n",
    "    new_observation, reward, done, info = env.step(int(action))\n",
    "    # return new_observation, reward, done, grads\n",
    "    return new_observation, info[\"score\"], done, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = list()\n",
    "    all_grads = list()\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = list()\n",
    "        current_grads = list()\n",
    "        observation = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            observation, reward, done, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    N = len(rewards)\n",
    "    for step in range(N - 2, -1, -1):\n",
    "        # a_n + a_n+1*gamma\n",
    "        discounted[step] = discounted[step] + discounted[step + 1] * discount_factor\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [10, 0, -50]\n",
    "discount_rewards(arr, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2])\n",
    "np.concatenate([x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = list()\n",
    "    for reward in all_rewards:\n",
    "        # discounted rewards\n",
    "        drs = discount_rewards(reward, discount_factor)\n",
    "        all_discounted_rewards.append(drs)\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "\n",
    "    normalize_rewards = list()\n",
    "    for discounted_rewards in all_discounted_rewards:\n",
    "        nrs = (discounted_rewards - reward_mean) / reward_std\n",
    "        normalize_rewards.append(nrs)\n",
    "    return normalize_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = [1,2,3]\n",
    "r2 = [-1,-2,3]\n",
    "all_rewards_1 = [r1, r2]\n",
    "list(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 4])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1,2,3], [3,4,5]]\n",
    "tf.reduce_mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/150 mean rewards: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susha\\AppData\\Local\\Temp\\ipykernel_15400\\1476244243.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  nrs = (discounted_rewards - reward_mean) / reward_std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2/150 mean rewards: 0.0\n",
      "Iteration: 3/150 mean rewards: 0.0\n",
      "Iteration: 4/150 mean rewards: 0.0\n",
      "Iteration: 5/150 mean rewards: 0.0\n",
      "Iteration: 6/150 mean rewards: 0.0\n",
      "Iteration: 7/150 mean rewards: 0.0\n",
      "Iteration: 8/150 mean rewards: 0.0\n",
      "Iteration: 9/150 mean rewards: 0.0\n",
      "Iteration: 10/150 mean rewards: 0.0\n",
      "Iteration: 11/150 mean rewards: 0.0\n",
      "Iteration: 12/150 mean rewards: 0.0\n",
      "Iteration: 13/150 mean rewards: 0.0\n",
      "Iteration: 14/150 mean rewards: 0.0\n",
      "Iteration: 15/150 mean rewards: 0.0\n",
      "Iteration: 16/150 mean rewards: 0.0\n",
      "Iteration: 17/150 mean rewards: 0.0\n",
      "Iteration: 18/150 mean rewards: 0.0\n",
      "Iteration: 19/150 mean rewards: 0.0\n",
      "Iteration: 20/150 mean rewards: 0.0\n",
      "Iteration: 21/150 mean rewards: 0.0\n",
      "Iteration: 22/150 mean rewards: 0.0\n",
      "Iteration: 23/150 mean rewards: 0.0\n",
      "Iteration: 24/150 mean rewards: 0.0\n",
      "Iteration: 25/150 mean rewards: 0.0\n",
      "Iteration: 26/150 mean rewards: 0.0\n",
      "Iteration: 27/150 mean rewards: 0.0\n",
      "Iteration: 28/150 mean rewards: 0.0\n",
      "Iteration: 29/150 mean rewards: 0.0\n",
      "Iteration: 30/150 mean rewards: 0.0\n",
      "Iteration: 31/150 mean rewards: 0.0\n",
      "Iteration: 32/150 mean rewards: 0.0\n",
      "Iteration: 33/150 mean rewards: 0.0\n",
      "Iteration: 34/150 mean rewards: 0.0\n",
      "Iteration: 35/150 mean rewards: 0.0\n",
      "Iteration: 36/150 mean rewards: 0.0\n",
      "Iteration: 37/150 mean rewards: 0.0\n",
      "Iteration: 38/150 mean rewards: 0.0\n",
      "Iteration: 39/150 mean rewards: 0.0\n",
      "Iteration: 40/150 mean rewards: 0.0\n",
      "Iteration: 41/150 mean rewards: 0.0\n",
      "Iteration: 42/150 mean rewards: 0.0\n",
      "Iteration: 43/150 mean rewards: 0.0\n",
      "Iteration: 44/150 mean rewards: 0.0\n",
      "Iteration: 45/150 mean rewards: 0.0\n",
      "Iteration: 46/150 mean rewards: 0.0\n",
      "Iteration: 47/150 mean rewards: 0.0\n",
      "Iteration: 48/150 mean rewards: 0.0\n",
      "Iteration: 49/150 mean rewards: 0.0\n",
      "Iteration: 50/150 mean rewards: 0.0\n",
      "Iteration: 51/150 mean rewards: 0.0\n",
      "Iteration: 52/150 mean rewards: 0.0\n",
      "Iteration: 53/150 mean rewards: 0.0\n",
      "Iteration: 54/150 mean rewards: 0.0\n",
      "Iteration: 55/150 mean rewards: 0.0\n",
      "Iteration: 56/150 mean rewards: 0.0\n",
      "Iteration: 57/150 mean rewards: 0.0\n",
      "Iteration: 58/150 mean rewards: 0.0\n",
      "Iteration: 59/150 mean rewards: 0.0\n",
      "Iteration: 60/150 mean rewards: 0.0\n",
      "Iteration: 61/150 mean rewards: 0.0\n",
      "Iteration: 62/150 mean rewards: 0.0\n",
      "Iteration: 63/150 mean rewards: 0.0\n",
      "Iteration: 64/150 mean rewards: 0.0\n",
      "Iteration: 65/150 mean rewards: 0.0\n",
      "Iteration: 66/150 mean rewards: 0.0\n",
      "Iteration: 67/150 mean rewards: 0.0\n",
      "Iteration: 68/150 mean rewards: 0.0\n",
      "Iteration: 69/150 mean rewards: 0.0\n",
      "Iteration: 70/150 mean rewards: 0.0\n",
      "Iteration: 71/150 mean rewards: 0.0\n",
      "Iteration: 72/150 mean rewards: 0.0\n",
      "Iteration: 73/150 mean rewards: 0.0\n",
      "Iteration: 74/150 mean rewards: 0.0\n",
      "Iteration: 75/150 mean rewards: 0.0\n",
      "Iteration: 76/150 mean rewards: 0.0\n",
      "Iteration: 77/150 mean rewards: 0.0\n",
      "Iteration: 78/150 mean rewards: 0.0\n",
      "Iteration: 79/150 mean rewards: 0.0\n",
      "Iteration: 80/150 mean rewards: 0.0\n",
      "Iteration: 81/150 mean rewards: 0.0\n",
      "Iteration: 82/150 mean rewards: 0.0\n",
      "Iteration: 83/150 mean rewards: 0.0\n",
      "Iteration: 84/150 mean rewards: 0.0\n",
      "Iteration: 85/150 mean rewards: 0.0\n",
      "Iteration: 86/150 mean rewards: 0.0\n",
      "Iteration: 87/150 mean rewards: 0.0\n",
      "Iteration: 88/150 mean rewards: 0.0\n",
      "Iteration: 89/150 mean rewards: 0.0\n",
      "Iteration: 90/150 mean rewards: 0.0\n",
      "Iteration: 91/150 mean rewards: 0.0\n",
      "Iteration: 92/150 mean rewards: 0.0\n",
      "Iteration: 93/150 mean rewards: 0.0\n",
      "Iteration: 94/150 mean rewards: 0.0\n",
      "Iteration: 95/150 mean rewards: 0.0\n",
      "Iteration: 96/150 mean rewards: 0.0\n",
      "Iteration: 97/150 mean rewards: 0.0\n",
      "Iteration: 98/150 mean rewards: 0.0\n",
      "Iteration: 99/150 mean rewards: 0.0\n",
      "Iteration: 100/150 mean rewards: 0.0\n",
      "Iteration: 101/150 mean rewards: 0.0\n",
      "Iteration: 102/150 mean rewards: 0.0\n",
      "Iteration: 103/150 mean rewards: 0.0\n",
      "Iteration: 104/150 mean rewards: 0.0\n",
      "Iteration: 105/150 mean rewards: 0.0\n",
      "Iteration: 106/150 mean rewards: 0.0\n",
      "Iteration: 107/150 mean rewards: 0.0\n",
      "Iteration: 108/150 mean rewards: 0.0\n",
      "Iteration: 109/150 mean rewards: 0.0\n",
      "Iteration: 110/150 mean rewards: 0.0\n",
      "Iteration: 111/150 mean rewards: 0.0\n",
      "Iteration: 112/150 mean rewards: 0.0\n",
      "Iteration: 113/150 mean rewards: 0.0\n",
      "Iteration: 114/150 mean rewards: 0.0\n",
      "Iteration: 115/150 mean rewards: 0.0\n",
      "Iteration: 116/150 mean rewards: 0.0\n",
      "Iteration: 117/150 mean rewards: 0.0\n",
      "Iteration: 118/150 mean rewards: 0.0\n",
      "Iteration: 119/150 mean rewards: 0.0\n",
      "Iteration: 120/150 mean rewards: 0.0\n",
      "Iteration: 121/150 mean rewards: 0.0\n",
      "Iteration: 122/150 mean rewards: 0.0\n",
      "Iteration: 123/150 mean rewards: 0.0\n",
      "Iteration: 124/150 mean rewards: 0.0\n",
      "Iteration: 125/150 mean rewards: 0.0\n",
      "Iteration: 126/150 mean rewards: 0.0\n",
      "Iteration: 127/150 mean rewards: 0.0\n",
      "Iteration: 128/150 mean rewards: 0.0\n",
      "Iteration: 129/150 mean rewards: 0.0\n",
      "Iteration: 130/150 mean rewards: 0.0\n",
      "Iteration: 131/150 mean rewards: 0.0\n",
      "Iteration: 132/150 mean rewards: 0.0\n",
      "Iteration: 133/150 mean rewards: 0.0\n",
      "Iteration: 134/150 mean rewards: 0.0\n",
      "Iteration: 135/150 mean rewards: 0.0\n",
      "Iteration: 136/150 mean rewards: 0.0\n",
      "Iteration: 137/150 mean rewards: 0.0\n",
      "Iteration: 138/150 mean rewards: 0.0\n",
      "Iteration: 139/150 mean rewards: 0.0\n",
      "Iteration: 140/150 mean rewards: 0.0\n",
      "Iteration: 141/150 mean rewards: 0.0\n",
      "Iteration: 142/150 mean rewards: 0.0\n",
      "Iteration: 143/150 mean rewards: 0.0\n",
      "Iteration: 144/150 mean rewards: 0.0\n",
      "Iteration: 145/150 mean rewards: 0.0\n",
      "Iteration: 146/150 mean rewards: 0.0\n",
      "Iteration: 147/150 mean rewards: 0.0\n",
      "Iteration: 148/150 mean rewards: 0.0\n",
      "Iteration: 149/150 mean rewards: 0.0\n",
      "Iteration: 150/150 mean rewards: 0.0\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn\n",
    "    )\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"\\rIteration: {iteration + 1}/{n_iterations}\",\n",
    "    f\"mean rewards: {total_rewards/n_episodes_per_update}\"\n",
    "    )\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = list()\n",
    "    # Weight of 5 hidden nodes, bias for 5 nodes, w for output node, bias for output node\n",
    "    N = len(model.trainable_variables)\n",
    "    for var_index in range(N):\n",
    "        temp_reduce_mean = list()\n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards): # rewards for every episode\n",
    "            for step, final_reward in enumerate(final_rewards): # several steps\n",
    "                result = final_reward * all_grads[episode_index][step][var_index]\n",
    "                temp_reduce_mean.append(result)\n",
    "        mean_grads = tf.reduce_mean(temp_reduce_mean, axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "model is saved as 'model_at_Fri_Jul_15_17_02_43_2022_.h5'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "unique_name = re.sub(r\"[\\s+:]\", \"_\", time.asctime())\n",
    "model_name = f\"model_at_{unique_name}_.h5\"\n",
    "model.save(model_name)\n",
    "print(f\"model is saved as '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -1), (2, -2), (3, 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = [1,2,3]\n",
    "r2 = [-1,-2,3]\n",
    "list(zip(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_model = tf.keras.models.load_model(\"model_at_Fri_Jul_15_16_52_58_2022_.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31, array([ 1.22569444, -0.31148438]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_one_episode(policy, model, n_max_steps=500, seed=42):\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        env.render()\n",
    "        action = policy(obs, model)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return step, obs\n",
    "\n",
    "show_one_episode(pg_policy, load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs, model):\n",
    "    PoleAngle = obs[2] \n",
    "    if PoleAngle < 0: # falling left\n",
    "        return 0 # Move left\n",
    "    return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Projects\\flappy-bird-gym\\test.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=0'>1</a>\u001b[0m show_one_episode(basic_policy, model)\n",
      "\u001b[1;32md:\\Documents\\Projects\\flappy-bird-gym\\test.ipynb Cell 27\u001b[0m in \u001b[0;36mshow_one_episode\u001b[1;34m(policy, model, n_max_steps, seed)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_max_steps):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=4'>5</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=5'>6</a>\u001b[0m     action \u001b[39m=\u001b[39m policy(obs, model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=6'>7</a>\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32md:\\Documents\\Projects\\flappy-bird-gym\\test.ipynb Cell 27\u001b[0m in \u001b[0;36mbasic_policy\u001b[1;34m(obs, model)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbasic_policy\u001b[39m(obs, model):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=1'>2</a>\u001b[0m     PoleAngle \u001b[39m=\u001b[39m obs[\u001b[39m2\u001b[39;49m] \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m PoleAngle \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# falling left\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Projects/flappy-bird-gym/test.ipynb#ch0000025?line=3'>4</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m# Move left\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "show_one_episode(basic_policy, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31, array([ 1.22569444, -0.34078125]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_one_episode(pg_policy, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('flap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d70d1ce5e45f62e811dcc7527d955328d8c66bcfd3266df5f93a66a3b503a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
